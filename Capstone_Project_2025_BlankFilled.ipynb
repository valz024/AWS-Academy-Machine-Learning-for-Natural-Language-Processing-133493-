{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6885c26f",
   "metadata": {},
   "source": [
    "# Capstone Project: Bringing It All Together\n",
    "\n",
    "In this lab, you will bring together many of the tools and techniques that you have learned throughout this course into a final project. You can choose from many different paths to get to the solution. You could use AWS Managed Services, such as Amazon Comprehend, or use the Amazon SageMaker models. Have fun on whichever path you choose.\n",
    "\n",
    "## Business scenario\n",
    "\n",
    "You work for a training organization that recently developed an introductory course about machine learning (ML). The course includes more than 40 videos that cover a broad range of ML topics. You have been asked to create an application that will students can use to quickly locate and view video content by searching for topics and key phrases.\n",
    "\n",
    "You have downloaded all of the videos to an Amazon Simple Storage Service (Amazon S3) bucket. Your assignment is to produce a dashboard that meets your supervisorâ€™s requirements.\n",
    "\n",
    "To assist you, all of the previous labs have been provided in this workspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06b330",
   "metadata": {},
   "source": [
    "## 1.5. 2. Transcribing the videos\n",
    "(Go to top)\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports & Configuration ---\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Lab region\n",
    "region_name = \"us-east-1\"\n",
    "\n",
    "# Student lab S3 bucket for outputs (provided by user)\n",
    "bucket = \"c176045a4549683l12324630t1w934798949390-labbucket-mmdkc0xqpkjx\"\n",
    "\n",
    "# Comprehend role (provided by user)\n",
    "job_data_access_role = \"arn:aws:iam::934798949390:role/service-role/c176045a4549683l12324630t1-ComprehendDataAccessRole-E2EeGxWSgfrW\"\n",
    "\n",
    "# Source videos (read-only shared bucket path for reference)\n",
    "video_source_bucket_prefix = \"s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/\"\n",
    "\n",
    "# AWS clients\n",
    "s3 = boto3.client(\"s3\", region_name=region_name)\n",
    "transcribe = boto3.client(\"transcribe\", region_name=region_name)\n",
    "comprehend = boto3.client(\"comprehend\", region_name=region_name)\n",
    "\n",
    "print(\"Region:\", region_name)\n",
    "print(\"Student bucket:\", bucket)\n",
    "print(\"Comprehend role:\", job_data_access_role)\n",
    "print(\"Video source:\", video_source_bucket_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Transcribing the videos ---\n",
    "# Most Academy roles restrict creating Transcribe jobs. This section prioritizes consuming\n",
    "# existing outputs in your student bucket (transcribe-job-*.json).\n",
    "# If no outputs are present, you can *optionally* attempt to start jobs (code commented below).\n",
    "\n",
    "# 1) (Optional) Peek at the shared source video listing (requires AWS CLI in system shell):\n",
    "# !aws s3 ls s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/\n",
    "\n",
    "# 2) Discover completed Transcribe JSONs in your bucket\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=\"transcribe-job-\")\n",
    "output_files = []\n",
    "for obj in resp.get(\"Contents\", []) or []:\n",
    "    key = obj[\"Key\"]\n",
    "    video_id = key.replace(\"transcribe-job-\", \"\").replace(\".json\", \"\")\n",
    "    output_files.append({\"Video\": video_id, \"OutputKey\": key})\n",
    "\n",
    "print(f\"Found {len(output_files)} Transcribe result files in s3://{bucket}/\")\n",
    "if not output_files:\n",
    "    print(\"No transcribe-job-*.json files found. If permitted, you may attempt to create jobs below.\")\n",
    "\n",
    "# 3) (Optional) Create Transcribe jobs (often blocked in student roles)\n",
    "# NOTE: This is provided for completeness; many labs expect AccessDenied for job creation.\n",
    "# from urllib.parse import quote\n",
    "# import uuid\n",
    "# media_uri = \"s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod01_Course Overview.mp4\"\n",
    "# job_name = f\"CapstoneTranscribeJob-{uuid.uuid4()}\"\n",
    "# try:\n",
    "#     start_resp = transcribe.start_transcription_job(\n",
    "#         TranscriptionJobName=job_name,\n",
    "#         Media={\"MediaFileUri\": media_uri},\n",
    "#         MediaFormat=\"mp4\",\n",
    "#         LanguageCode=\"en-US\",\n",
    "#         OutputBucketName=bucket\n",
    "#     )\n",
    "#     print(\"Started job:\", start_resp[\"TranscriptionJob\"][\"TranscriptionJobName\"])\n",
    "# except Exception as e:\n",
    "#     print(\"Transcribe start likely not permitted in this lab:\", e)\n",
    "\n",
    "output_files[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ca35e",
   "metadata": {},
   "source": [
    "## 1.6. 3. Normalizing the text\n",
    "(Go to top)\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load transcripts and normalize text ---\n",
    "data_rows = []\n",
    "for entry in output_files:\n",
    "    key = entry[\"OutputKey\"]\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        payload = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        data = json.loads(payload)\n",
    "        transcript = data[\"results\"][\"transcripts\"][0][\"transcript\"]\n",
    "        data_rows.append({\"Video\": entry[\"Video\"], \"Transcription\": transcript})\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {key}:\", e)\n",
    "\n",
    "df = pd.DataFrame(data_rows)\n",
    "print(\"Loaded transcripts:\", len(df))\n",
    "display(df.head())\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "if \"Transcription\" not in df.columns:\n",
    "    raise KeyError(\"Expected 'Transcription' column missing; verify previous step.\")\n",
    "\n",
    "df[\"clean_text\"] = df[\"Transcription\"].apply(normalize_text)\n",
    "print(\"Normalized transcripts.\")\n",
    "display(df[[\"Video\", \"clean_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e3e26",
   "metadata": {},
   "source": [
    "## 1.7. 4. Extracting key phrases and topics\n",
    "(Go to top)\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc902fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Extracting key phrases and topics (real Amazon Comprehend) ---\n",
    "\n",
    "# 1) DetectKeyPhrases per transcript (real-time)\n",
    "key_rows = []\n",
    "print(\"Running Amazon Comprehend DetectKeyPhrases...\")\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"Transcription\"][:4500]  # ~5000 bytes limit safety\n",
    "    try:\n",
    "        resp = comprehend.detect_key_phrases(Text=text, LanguageCode=\"en\")\n",
    "        phrases = [kp[\"Text\"] for kp in resp.get(\"KeyPhrases\", [])]\n",
    "    except Exception as e:\n",
    "        print(f\"Comprehend error on {row['Video']}: {e}\")\n",
    "        phrases = []\n",
    "    key_rows.append({\"Video\": row[\"Video\"], \"KeyPhrases\": phrases})\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i+1}/{len(df)}\")\n",
    "    sleep(0.25)  # polite throttle\n",
    "\n",
    "df_keys = pd.DataFrame(key_rows)\n",
    "print(\"Key phrases extracted for:\", len(df_keys), \"videos\")\n",
    "display(df_keys.head())\n",
    "\n",
    "# 2) Prepare inputs for Topics Detection (Phase Detection)\n",
    "input_prefix = \"transcribe-json-input/\"\n",
    "uploaded = 0\n",
    "for _, r in df.iterrows():\n",
    "    key = f\"{input_prefix}{r['Video']}.txt\"\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=r[\"Transcription\"].encode(\"utf-8\"))\n",
    "    uploaded += 1\n",
    "print(f\"Uploaded {uploaded} ONE_DOC_PER_FILE inputs to s3://{bucket}/{input_prefix}\")\n",
    "\n",
    "# 3) Start Topics Detection Job\n",
    "input_s3_uri  = f\"s3://{bucket}/{input_prefix}\"\n",
    "output_s3_uri = f\"s3://{bucket}/comprehend-topics-output/\"\n",
    "\n",
    "try:\n",
    "    start = comprehend.start_topics_detection_job(\n",
    "        InputDataConfig={\"S3Uri\": input_s3_uri, \"InputFormat\": \"ONE_DOC_PER_FILE\"},\n",
    "        OutputDataConfig={\"S3Uri\": output_s3_uri},\n",
    "        DataAccessRoleArn=job_data_access_role,\n",
    "        JobName=f\"ComprehendTopics-{uuid.uuid4()}\",\n",
    "        NumberOfTopics=10\n",
    "    )\n",
    "    job_id = start[\"JobId\"]\n",
    "    print(\"Started Topics Detection Job:\", job_id)\n",
    "except Exception as e:\n",
    "    job_id = None\n",
    "    print(\"Could not start Topics Detection (may be restricted by lab role):\", e)\n",
    "\n",
    "# 4) Monitor (if started)\n",
    "if job_id:\n",
    "    while True:\n",
    "        desc = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "        props = desc[\"TopicsDetectionJobProperties\"]\n",
    "        state = props[\"JobStatus\"]\n",
    "        print(\"Job status:\", state)\n",
    "        if state in (\"COMPLETED\", \"FAILED\"):\n",
    "            print(\"Final properties:\", json.dumps(props, indent=2, default=str))\n",
    "            break\n",
    "        time.sleep(60)\n",
    "\n",
    "# 5) List outputs (if any)\n",
    "res = s3.list_objects_v2(Bucket=bucket, Prefix=\"comprehend-topics-output/\")\n",
    "print(\"Topics output files:\")\n",
    "for obj in res.get(\"Contents\", []) or []:\n",
    "    print(\"-\", obj[\"Key\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc2bf6",
   "metadata": {},
   "source": [
    "## 1.8. 5. Creating the dashboard\n",
    "(Go to top)\n",
    "\n",
    "Use this section to create the dashboard for your solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5874df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Creating the dashboard (simple visualization example) ---\n",
    "# Here we visualize the top key phrases across all transcripts.\n",
    "# In a fuller dashboard, you'd add filters/search, per-video links, etc.\n",
    "\n",
    "if not df_keys.empty and \"KeyPhrases\" in df_keys.columns:\n",
    "    all_phrases = [p for sub in df_keys[\"KeyPhrases\"] for p in (sub or [])]\n",
    "    if all_phrases:\n",
    "        top = Counter(all_phrases).most_common(10)\n",
    "        labels, counts = zip(*top)\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.barh(labels, counts)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(\"Top Key Phrases (Amazon Comprehend)\")\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No key phrases found to visualize.\")\n",
    "else:\n",
    "    print(\"df_keys is empty; run extraction first.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}