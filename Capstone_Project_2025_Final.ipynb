{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ac6283",
   "metadata": {},
   "source": [
    "# Capstone Project 2025 ‚Äî Unified AWS Version (Grading-Ready)\n",
    "\n",
    "This notebook **replaces** the template contents and provides a start-to-finish pipeline:\n",
    "1) Load Transcribe results from S3  \n",
    "2) Normalize text  \n",
    "3) Extract **real** key phrases using **Amazon Comprehend (DetectKeyPhrases)**  \n",
    "4) Prepare input files and run **Comprehend Topics Detection (Phase Detection)**  \n",
    "5) Basic visualizations\n",
    "\n",
    "> **Region:** `us-east-1`  \n",
    "> **Comprehend Role:** `arn:aws:iam::934798949390:role/service-role/c176045a4549683l12324630t1-ComprehendDataAccessRole-E2EeGxWSgfrW`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# AWS region & role from template\n",
    "AWS_REGION = \"us-east-1\"\n",
    "COMPREHEND_DATA_ACCESS_ROLE = \"arn:aws:iam::934798949390:role/service-role/c176045a4549683l12324630t1-ComprehendDataAccessRole-E2EeGxWSgfrW\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- REQUIRED: set your S3 bucket that holds Transcribe outputs ---\n",
    "# If you ran Transcribe already, your result JSON files should be in this bucket.\n",
    "output_bucket = \"c176045a4549683l12324630t1w510414224130-labbucket-ymkoanalkg8l\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=AWS_REGION)\n",
    "transcribe = boto3.client(\"transcribe\", region_name=AWS_REGION)\n",
    "comprehend = boto3.client(\"comprehend\", region_name=AWS_REGION)\n",
    "\n",
    "print(\"Using region:\", AWS_REGION)\n",
    "print(\"Data access role:\", COMPREHEND_DATA_ACCESS_ROLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03491258",
   "metadata": {},
   "source": [
    "## 1) Discover completed Transcribe JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fba99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = s3.list_objects_v2(Bucket=output_bucket, Prefix=\"transcribe-job-\")\n",
    "output_files = []\n",
    "\n",
    "for obj in response.get(\"Contents\", []):\n",
    "    key = obj[\"Key\"]\n",
    "    # Infer a simple video id from the key (UUID or slug)\n",
    "    video_name = key.replace(\"transcribe-job-\", \"\").replace(\".json\", \"\")\n",
    "    output_files.append({\"Video\": video_name, \"OutputKey\": key})\n",
    "\n",
    "print(f\"‚úÖ Found {len(output_files)} Transcribe result files.\")\n",
    "if len(output_files) == 0:\n",
    "    print(\"‚ö†Ô∏è No transcribe-job-*.json files found. Confirm your bucket name and prefix.\")\n",
    "output_files[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea830d3",
   "metadata": {},
   "source": [
    "## 2) Load transcripts from S3 into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec717eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_rows = []\n",
    "\n",
    "for entry in output_files:\n",
    "    key = entry[\"OutputKey\"]\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=output_bucket, Key=key)\n",
    "        data = json.loads(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "        transcript = data[\"results\"][\"transcripts\"][0][\"transcript\"]\n",
    "        data_rows.append({\"Video\": entry[\"Video\"], \"Transcription\": transcript})\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {key}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data_rows)\n",
    "print(f\"‚úÖ Loaded {len(df)} transcripts.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672b4e2",
   "metadata": {},
   "source": [
    "## 3) Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2804a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "if \"Transcription\" not in df.columns:\n",
    "    raise KeyError(\"Expected 'Transcription' column not found. Check previous step.\")\n",
    "\n",
    "df[\"clean_text\"] = df[\"Transcription\"].apply(normalize_text)\n",
    "print(\"‚úÖ Text normalized.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a954f",
   "metadata": {},
   "source": [
    "## 4) Extract **real** key phrases with Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c421410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_rows = []\n",
    "\n",
    "print(\"üîç Starting Amazon Comprehend DetectKeyPhrases...\")\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"Transcription\"][:4500]  # Comprehend ~5000 bytes per call\n",
    "    try:\n",
    "        response = comprehend.detect_key_phrases(Text=text, LanguageCode=\"en\")\n",
    "        phrases = [kp[\"Text\"] for kp in response.get(\"KeyPhrases\", [])]\n",
    "        key_rows.append({\"Video\": row[\"Video\"], \"KeyPhrases\": phrases})\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{len(df)} transcripts...\")\n",
    "        sleep(0.25)  # throttle to avoid API limits\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error on {row['Video']}: {e}\")\n",
    "        key_rows.append({\"Video\": row[\"Video\"], \"KeyPhrases\": []})\n",
    "\n",
    "df_keys = pd.DataFrame(key_rows)\n",
    "print(f\"‚úÖ Extracted key phrases for {len(df_keys)} transcripts.\")\n",
    "df_keys.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009f6e1",
   "metadata": {},
   "source": [
    "## 5) Combine transcripts and key phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_combined = pd.merge(df, df_keys, on=\"Video\", how=\"left\")\n",
    "print(\"‚úÖ Combined DataFrame shape:\", df_combined.shape)\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ec491",
   "metadata": {},
   "source": [
    "## 6) Prepare inputs for **Comprehend Topics Detection** (Phase Detection)\n",
    "We will upload one plain-text file per transcript to `s3://<bucket>/transcribe-json-input/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_prefix = \"transcribe-json-input/\"  # ONE_DOC_PER_FILE input\n",
    "uploaded = 0\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    key = f\"{input_prefix}{r['Video']}.txt\"\n",
    "    body = r[\"Transcription\"].encode(\"utf-8\")\n",
    "    s3.put_object(Bucket=output_bucket, Key=key, Body=body)\n",
    "    uploaded += 1\n",
    "\n",
    "print(f\"‚úÖ Uploaded {uploaded} input text files to s3://{output_bucket}/{input_prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7242864",
   "metadata": {},
   "source": [
    "## 7) Start Comprehend Topics Detection Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_s3_uri  = f\"s3://{output_bucket}/transcribe-json-input/\"\n",
    "output_s3_uri = f\"s3://{output_bucket}/comprehend-topics-output/\"\n",
    "\n",
    "response = comprehend.start_topics_detection_job(\n",
    "    InputDataConfig={\"S3Uri\": input_s3_uri, \"InputFormat\": \"ONE_DOC_PER_FILE\"},\n",
    "    OutputDataConfig={\"S3Uri\": output_s3_uri},\n",
    "    DataAccessRoleArn=COMPREHEND_DATA_ACCESS_ROLE,\n",
    "    JobName=f\"ComprehendTopics-{uuid.uuid4()}\",\n",
    "    NumberOfTopics=10\n",
    ")\n",
    "\n",
    "job_id = response[\"JobId\"]\n",
    "print(\"üöÄ Started Topics Detection Job:\", job_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ca56",
   "metadata": {},
   "source": [
    "## 8) Monitor job status until completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210be1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    status = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    "    props = status[\"TopicsDetectionJobProperties\"]\n",
    "    state = props[\"JobStatus\"]\n",
    "    print(f\"Job status: {state}\")\n",
    "    if state in (\"COMPLETED\", \"FAILED\"):\n",
    "        print(\"Final properties:\", json.dumps(props, indent=2, default=str))\n",
    "        break\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6329f1",
   "metadata": {},
   "source": [
    "## 9) List Topics Detection outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = s3.list_objects_v2(Bucket=output_bucket, Prefix=\"comprehend-topics-output/\")\n",
    "files = [obj[\"Key\"] for obj in result.get(\"Contents\", [])]\n",
    "print(\"Output files:\")\n",
    "for k in files:\n",
    "    print(\"-\", k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5323189",
   "metadata": {},
   "source": [
    "## 10) Quick visualization of top key phrases (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(df_keys) and len(df_keys['KeyPhrases'].dropna()):\n",
    "    all_phrases = [p for sub in df_keys[\"KeyPhrases\"] for p in (sub or [])]\n",
    "    if all_phrases:\n",
    "        top = Counter(all_phrases).most_common(10)\n",
    "        words, counts = zip(*top)\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.barh(words, counts)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(\"Top Key Phrases (Amazon Comprehend)\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No phrases to visualize.\")\n",
    "else:\n",
    "    print(\"Key phrases DataFrame is empty or missing.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}