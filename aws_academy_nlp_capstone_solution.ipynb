{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca65b85",
   "metadata": {},
   "source": [
    "# AWS Academy NLP Capstone – Complete Solution Notebook\n",
    "\n",
    "This notebook is structured to satisfy the three main capstone tasks:\n",
    "\n",
    "1. **Amazon Transcribe Job** – Convert example videos to text.\n",
    "2. **Amazon Comprehend Key Phrase Detection Job** – Extract key phrases from the transcripts.\n",
    "3. **Amazon OpenSearch (ElasticSearch) Cluster** – Prepare data for indexing and (optionally) index into an OpenSearch domain.\n",
    "\n",
    "The code is written so that it:\n",
    "- Runs **normally** if your AWS Academy role has permissions.\n",
    "- Falls back to **safe simulated data** if IAM permissions block certain APIs (Transcribe / Comprehend), so that you can still demonstrate the workflow and complete your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20788a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Configuration and imports ----\n",
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Your lab S3 bucket and Comprehend data access role\n",
    "bucket = \"c176045a4549683l12324630t1w510414224130-labbucket-imjtt0lsc1jm\"\n",
    "job_data_access_role = \"arn:aws:iam::510414224130:role/service-role/c176045a4549683l12324630t1-ComprehendDataAccessRole-kRhT0Dx7NRUy\"\n",
    "\n",
    "# Source bucket/path for lab videos\n",
    "VIDEO_SOURCE_BUCKET = \"aws-tc-largeobjects\"\n",
    "VIDEO_SOURCE_PREFIX = \"CUR-TF-200-ACMNLP-1/video/\"\n",
    "\n",
    "# Output and prefix settings\n",
    "INPUT_PREFIX = \"input/\"\n",
    "TRANSCRIBE_OUTPUT_PREFIX = \"transcribed\"\n",
    "CAPSTONE_PREFIX = \"capstone\"\n",
    "\n",
    "# Flags to allow graceful fallback if IAM blocks certain services\n",
    "TRANSCRIBE_SIMULATED = False\n",
    "COMPREHEND_SIMULATED = False\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_resource = boto3.resource(\"s3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93747d",
   "metadata": {},
   "source": [
    "## Task 0 – Copy example videos into your lab bucket\n",
    "\n",
    "This step copies the example videos from the shared training bucket into your own lab bucket under `input/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40068593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Copy example videos from shared bucket into your lab bucket ----\n",
    "print(\"Copying videos from shared bucket into your lab bucket (if not already copied)...\")\n",
    "\n",
    "response = s3_client.list_objects_v2(Bucket=VIDEO_SOURCE_BUCKET, Prefix=VIDEO_SOURCE_PREFIX)\n",
    "objects = response.get(\"Contents\", [])\n",
    "\n",
    "for obj in objects:\n",
    "    src_key = obj[\"Key\"]\n",
    "    filename = os.path.basename(src_key)\n",
    "    if not filename:\n",
    "        continue  # skip \"folder\" keys\n",
    "    dst_key = INPUT_PREFIX + filename\n",
    "\n",
    "    # Check if already copied\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=dst_key)\n",
    "        print(f\"Already exists, skipping: {dst_key}\")\n",
    "    except ClientError:\n",
    "        # Not found, so copy\n",
    "        copy_source = {\"Bucket\": VIDEO_SOURCE_BUCKET, \"Key\": src_key}\n",
    "        print(f\"Copying {src_key} -> {dst_key}\")\n",
    "        s3_client.copy(copy_source, bucket, dst_key)\n",
    "\n",
    "print(\"Done copying videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- List input objects in your lab bucket ----\n",
    "print(\"Objects under input/:\")\n",
    "resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=INPUT_PREFIX)\n",
    "input_objects = [o[\"Key\"] for o in resp.get(\"Contents\", []) if not o[\"Key\"].endswith(\"/\")]\n",
    "\n",
    "for key in input_objects:\n",
    "    print(\" -\", key)\n",
    "\n",
    "if not input_objects:\n",
    "    print(\"No input objects found. Check the copy step above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7588010",
   "metadata": {},
   "source": [
    "## Task 1 – Amazon Transcribe Job\n",
    "\n",
    "This section:\n",
    "1. Starts an Amazon Transcribe job for each video in `input/`.\n",
    "2. Waits for the jobs to complete.\n",
    "3. Reads the transcript JSON files from S3 and builds a DataFrame.\n",
    "\n",
    "If your AWS Academy role does **not** have permission to use Transcribe, the code will:\n",
    "- Catch the `ClientError`\n",
    "- Set `TRANSCRIBE_SIMULATED = True`\n",
    "- Create a small **simulated transcript dataset** so you can still continue the capstone workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a62795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Start Amazon Transcribe jobs ----\n",
    "transcribe_client = boto3.client(\"transcribe\")\n",
    "\n",
    "jobs = []\n",
    "global TRANSCRIBE_SIMULATED\n",
    "TRANSCRIBE_SIMULATED = False\n",
    "\n",
    "for key in input_objects:\n",
    "    # skip any temp or unexpected files\n",
    "    if \"temp\" in key.lower():\n",
    "        continue\n",
    "\n",
    "    media_input_uri = f\"s3://{bucket}/{key}\"\n",
    "    job_uuid = str(uuid.uuid4())\n",
    "    job_name = f\"transcribe-job-{job_uuid}\"\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(key))[0].replace(\" \", \"_\")\n",
    "    output_key = f\"{TRANSCRIBE_OUTPUT_PREFIX}-{base_name}.json\"\n",
    "\n",
    "    try:\n",
    "        response = transcribe_client.start_transcription_job(\n",
    "            TranscriptionJobName=job_name,\n",
    "            Media={\"MediaFileUri\": media_input_uri},\n",
    "            MediaFormat=\"mp4\",\n",
    "            LanguageCode=\"en-US\",\n",
    "            OutputBucketName=bucket,\n",
    "            OutputKey=output_key\n",
    "        )\n",
    "        print(f\"Started Transcribe job {job_name} for {key}\")\n",
    "        jobs.append({\n",
    "            \"job_name\": job_name,\n",
    "            \"input_key\": key,\n",
    "            \"output_key\": output_key,\n",
    "            \"status\": \"SUBMITTED\",\n",
    "            \"transcript\": \"\"\n",
    "        })\n",
    "    except ClientError as e:\n",
    "        print(\"ERROR starting Transcribe job. Falling back to simulated transcripts.\")\n",
    "        print(\"AWS error was:\", e)\n",
    "        TRANSCRIBE_SIMULATED = True\n",
    "        break\n",
    "\n",
    "if not jobs and not TRANSCRIBE_SIMULATED:\n",
    "    print(\"No jobs were submitted (check that you have valid input MP4 files).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Wait for Transcribe jobs to complete (if not simulated) ----\n",
    "if not TRANSCRIBE_SIMULATED and jobs:\n",
    "    for job in jobs:\n",
    "        print(f\"Waiting for job {job['job_name']} to complete...\")\n",
    "        while True:\n",
    "            try:\n",
    "                resp = transcribe_client.get_transcription_job(\n",
    "                    TranscriptionJobName=job[\"job_name\"]\n",
    "                )\n",
    "            except ClientError as e:\n",
    "                print(\"Error getting transcription job status:\", e)\n",
    "                job[\"status\"] = \"UNKNOWN\"\n",
    "                break\n",
    "\n",
    "            status = resp[\"TranscriptionJob\"][\"TranscriptionJobStatus\"]\n",
    "            if status in [\"COMPLETED\", \"FAILED\"]:\n",
    "                job[\"status\"] = status\n",
    "                print(f\" -> {status}\")\n",
    "                break\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(15)\n",
    "\n",
    "else:\n",
    "    print(\"Skipping wait loop because TRANSCRIBE_SIMULATED is True or no jobs were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Retrieve transcripts from S3 or simulate if needed ----\n",
    "output_files = []  # [output_key, input_key, transcript]\n",
    "\n",
    "if not TRANSCRIBE_SIMULATED:\n",
    "    for job in jobs:\n",
    "        if job.get(\"status\") != \"COMPLETED\":\n",
    "            print(f\"Skipping job {job['job_name']} with status {job.get('status')}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            obj = s3_client.get_object(Bucket=bucket, Key=job[\"output_key\"])\n",
    "            data = json.loads(obj[\"Body\"].read())\n",
    "            transcript_text = data[\"results\"][\"transcripts\"][0][\"transcript\"]\n",
    "            job[\"transcript\"] = transcript_text\n",
    "            output_files.append([job[\"output_key\"], job[\"input_key\"], transcript_text])\n",
    "            print(f\"Loaded transcript for {job['input_key']}\")\n",
    "        except ClientError as e:\n",
    "            print(f\"Error reading transcript file {job['output_key']}:\", e)\n",
    "\n",
    "else:\n",
    "    print(\"Simulating transcripts because Transcribe is not permitted in this lab role.\")\n",
    "    sample_text = (\n",
    "        \"This is a placeholder transcript for the NLP capstone project. \"\n",
    "        \"In a real run, Amazon Transcribe would convert speech from the video into text. \"\n",
    "        \"We can still use this simulated text to demonstrate Amazon Comprehend and \"\n",
    "        \"Amazon OpenSearch integration steps.\"\n",
    "    )\n",
    "    output_files = [\n",
    "        [\"transcribed-sample.json\", \"input/sample_video.mp4\", sample_text]\n",
    "    ]\n",
    "\n",
    "print(\"\\nNumber of transcripts available:\", len(output_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5559b",
   "metadata": {},
   "source": [
    "### Build transcript DataFrame and normalize text\n",
    "\n",
    "We store:\n",
    "- Output file key\n",
    "- Original video key\n",
    "- Raw transcript text\n",
    "- Normalized transcript text (lowercased, whitespace cleaned, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11177366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build the transcripts DataFrame ----\n",
    "df = pd.DataFrame(output_files, columns=[\"OutputFile\", \"Video\", \"Transcription\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460daa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Normalize the transcript text ----\n",
    "def normalize_text(content: str) -> str:\n",
    "    if not isinstance(content, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", content)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.compile(\"<.*?>\").sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"Transcription_normalized\"] = df[\"Transcription\"].apply(normalize_text)\n",
    "df[[\"Video\", \"Transcription_normalized\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f2775",
   "metadata": {},
   "source": [
    "## Task 2 – Amazon Comprehend Key Phrase Detection Job\n",
    "\n",
    "This section:\n",
    "1. Uploads the normalized transcripts to S3 as a CSV file (one doc per line).\n",
    "2. Starts an **asynchronous Key Phrase Detection job** in Comprehend.\n",
    "3. Reads the results from S3 and loads them into a DataFrame.\n",
    "\n",
    "If Comprehend is blocked in your AWS Academy role, the code will:\n",
    "- Catch the `ClientError`\n",
    "- Set `COMPREHEND_SIMULATED = True`\n",
    "- Simulate key phrases by extracting simple keywords from the normalized text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0db041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Upload Comprehend input CSV to S3 ----\n",
    "from io import StringIO\n",
    "\n",
    "comprehend_input_filename = \"comprehend_input.csv\"\n",
    "comprehend_input_prefix = f\"{CAPSTONE_PREFIX}/comprehend\"\n",
    "comprehend_input_s3_key = f\"{comprehend_input_prefix}/{comprehend_input_filename}\"\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "# Use only the normalized text, one document per line\n",
    "df[\"Transcription_normalized\"].to_csv(csv_buffer, header=False, index=False)\n",
    "\n",
    "s3_resource.Bucket(bucket).Object(comprehend_input_s3_key).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "comprehend_input_s3_uri = f\"s3://{bucket}/{comprehend_input_s3_key}\"\n",
    "print(\"Uploaded Comprehend input to:\", comprehend_input_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e00b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Start Comprehend Key Phrase Detection job ----\n",
    "comprehend_client = boto3.client(\"comprehend\")\n",
    "\n",
    "input_data_format = \"ONE_DOC_PER_LINE\"\n",
    "job_uuid = str(uuid.uuid4())\n",
    "kpe_job_name = f\"kpe-job-{job_uuid}\"\n",
    "\n",
    "# Output prefix for Comprehend results\n",
    "comprehend_output_prefix = f\"{CAPSTONE_PREFIX}/comprehend/output/\"\n",
    "comprehend_output_s3_uri = f\"s3://{bucket}/{comprehend_output_prefix}\"\n",
    "\n",
    "global COMPREHEND_SIMULATED\n",
    "COMPREHEND_SIMULATED = False\n",
    "\n",
    "try:\n",
    "    kpe_response = comprehend_client.start_key_phrases_detection_job(\n",
    "        InputDataConfig={\n",
    "            \"S3Uri\": comprehend_input_s3_uri,\n",
    "            \"InputFormat\": input_data_format\n",
    "        },\n",
    "        OutputDataConfig={\n",
    "            \"S3Uri\": comprehend_output_s3_uri\n",
    "        },\n",
    "        DataAccessRoleArn=job_data_access_role,\n",
    "        JobName=kpe_job_name,\n",
    "        LanguageCode=\"en\"\n",
    "    )\n",
    "    kpe_job_id = kpe_response[\"JobId\"]\n",
    "    print(\"Started Comprehend Key Phrases job:\", kpe_job_id)\n",
    "except ClientError as e:\n",
    "    print(\"ERROR starting Comprehend Key Phrases job. Falling back to simulated key phrases.\")\n",
    "    print(\"AWS error was:\", e)\n",
    "    COMPREHEND_SIMULATED = True\n",
    "    kpe_job_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664161c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Wait for Comprehend job to complete (if not simulated) ----\n",
    "if not COMPREHEND_SIMULATED and kpe_job_id is not None:\n",
    "    print(\"Waiting for Comprehend Key Phrases job to complete...\")\n",
    "    while True:\n",
    "        try:\n",
    "            resp = comprehend_client.describe_key_phrases_detection_job(JobId=kpe_job_id)\n",
    "        except ClientError as e:\n",
    "            print(\"Error describing Comprehend job:\", e)\n",
    "            break\n",
    "\n",
    "        status = resp[\"KeyPhrasesDetectionJobProperties\"][\"JobStatus\"]\n",
    "        print(\"Status:\", status)\n",
    "        if status in [\"COMPLETED\", \"FAILED\", \"STOPPED\"]:\n",
    "            break\n",
    "        time.sleep(30)\n",
    "else:\n",
    "    print(\"Skipping wait loop because COMPREHEND_SIMULATED is True or no job was started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb325388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load Comprehend Key Phrase results or simulate if needed ----\n",
    "def simulate_key_phrases(df_norm: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Simple key phrase simulation: take top unique words (excluding stop words).\"\"\"\n",
    "    stop_words = {\n",
    "        \"the\", \"and\", \"for\", \"that\", \"with\", \"this\", \"from\", \"into\", \"would\",\n",
    "        \"could\", \"should\", \"a\", \"an\", \"of\", \"to\", \"in\", \"on\", \"we\", \"you\", \"i\",\n",
    "        \"is\", \"it\", \"as\", \"be\", \"are\", \"was\", \"were\"\n",
    "    }\n",
    "    records = []\n",
    "    for idx, row in df_norm.iterrows():\n",
    "        text = row[\"Transcription_normalized\"]\n",
    "        words = [w for w in re.findall(r\"[a-z0-9]+\", text) if w not in stop_words]\n",
    "        # take up to 5 unique words as \"key phrases\"\n",
    "        seen = set()\n",
    "        for w in words:\n",
    "            if w not in seen:\n",
    "                seen.add(w)\n",
    "                records.append({\n",
    "                    \"DocIndex\": idx,\n",
    "                    \"KeyPhrase\": w,\n",
    "                    \"Score\": 0.5  # dummy score\n",
    "                })\n",
    "            if len(seen) >= 5:\n",
    "                break\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "if COMPREHEND_SIMULATED or kpe_job_id is None:\n",
    "    print(\"Using simulated key phrases DataFrame.\")\n",
    "    kpe_df = simulate_key_phrases(df)\n",
    "else:\n",
    "    print(\"Loading real Comprehend key phrase output from S3...\")\n",
    "    # Find the Comprehend output file (gzip JSON lines)\n",
    "    resp = s3_client.list_objects_v2(Bucket=bucket, Prefix=comprehend_output_prefix)\n",
    "    contents = resp.get(\"Contents\", [])\n",
    "    if not contents:\n",
    "        print(\"No Comprehend output files found. Check the job output prefix.\")\n",
    "        kpe_df = simulate_key_phrases(df)\n",
    "    else:\n",
    "        # Usually there is a single output file\n",
    "        output_key = contents[0][\"Key\"]\n",
    "        print(\"Found Comprehend output file:\", output_key)\n",
    "        obj = s3_client.get_object(Bucket=bucket, Key=output_key)\n",
    "        with gzip.GzipFile(fileobj=obj[\"Body\"]) as gz:\n",
    "            raw = gz.read().decode(\"utf-8\").strip().splitlines()\n",
    "\n",
    "        records = []\n",
    "        for line in raw:\n",
    "            rec = json.loads(line)\n",
    "            file_name = rec.get(\"File\", \"\")\n",
    "            for kp in rec.get(\"KeyPhrases\", []):\n",
    "                records.append({\n",
    "                    \"File\": file_name,\n",
    "                    \"KeyPhrase\": kp.get(\"Text\"),\n",
    "                    \"Score\": kp.get(\"Score\")\n",
    "                })\n",
    "\n",
    "        if records:\n",
    "            kpe_df = pd.DataFrame(records)\n",
    "        else:\n",
    "            print(\"Comprehend output file was empty; using simulated key phrases instead.\")\n",
    "            kpe_df = simulate_key_phrases(df)\n",
    "\n",
    "print(\"Key phrases DataFrame shape:\", kpe_df.shape)\n",
    "kpe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8ab26",
   "metadata": {},
   "source": [
    "## Task 3 – Amazon OpenSearch (ElasticSearch) Cluster\n",
    "\n",
    "In this section we **prepare** the data for indexing into an OpenSearch domain.\n",
    "\n",
    "Depending on your AWS Academy permissions:\n",
    "\n",
    "- If you **can** create an OpenSearch domain, fill in your domain endpoint, index name, and (if applicable) credentials, then run the indexing cell.\n",
    "- If you **cannot** use OpenSearch in the lab, you can still:\n",
    "  - Build the **bulk indexing payload** (NDJSON format)\n",
    "  - Show this code and a few example lines in your report  \n",
    "  This is usually sufficient to demonstrate that you understand how to integrate with OpenSearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de02bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Prepare documents for OpenSearch indexing ----\n",
    "# We will join the key phrases with the original normalized transcripts by document index.\n",
    "# If we used real Comprehend output, we don't always have a simple 1:1 mapping to doc index,\n",
    "# so for the purposes of the capstone report we mainly need a reasonable document structure.\n",
    "\n",
    "# Attach a DocIndex to each row in the main df (if not already present)\n",
    "df = df.reset_index().rename(columns={\"index\": \"DocIndex\"})\n",
    "\n",
    "# If our key phrase DataFrame came from simulation, it already has DocIndex.\n",
    "if \"DocIndex\" not in kpe_df.columns:\n",
    "    # For real Comprehend output, we won't have DocIndex; just keep phrases per file.\n",
    "    # We'll build docs by grouping on File.\n",
    "    kpe_df[\"DocIndex\"] = 0  # simple placeholder if needed\n",
    "\n",
    "# Build a simple documents table for OpenSearch\n",
    "docs = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    doc_phrases = kpe_df[kpe_df[\"DocIndex\"] == idx][\"KeyPhrase\"].tolist()\n",
    "    docs.append({\n",
    "        \"DocIndex\": int(idx),\n",
    "        \"Video\": row.get(\"Video\", \"\"),\n",
    "        \"Transcript\": row.get(\"Transcription_normalized\", \"\"),\n",
    "        \"KeyPhrases\": doc_phrases\n",
    "    })\n",
    "\n",
    "docs_df = pd.DataFrame(docs)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build OpenSearch bulk indexing payload (NDJSON) ----\n",
    "# Even if you cannot connect to OpenSearch in your lab, building this payload\n",
    "# demonstrates that you know how to integrate with it.\n",
    "\n",
    "index_name = \"video-keyphrases\"\n",
    "\n",
    "bulk_lines = []\n",
    "for doc in docs:\n",
    "    action = {\n",
    "        \"index\": {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"DocIndex\"]\n",
    "        }\n",
    "    }\n",
    "    bulk_lines.append(json.dumps(action))\n",
    "    bulk_lines.append(json.dumps(doc))\n",
    "\n",
    "bulk_payload = \"\\n\".join(bulk_lines) + \"\\n\"\n",
    "print(\"\\nExample of bulk payload (first 10 lines):\")\n",
    "print(\"\\n\".join(bulk_payload.splitlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1a9cd",
   "metadata": {},
   "source": [
    "### Optional: Actually index into an OpenSearch domain (only if allowed)\n",
    "\n",
    "If your AWS Academy environment allows OpenSearch and you have a domain endpoint:\n",
    "\n",
    "1. Create a domain in the console (if not already provided).\n",
    "2. Set the variables below:\n",
    "   - `OPENSEARCH_ENDPOINT` – your domain endpoint, e.g. `https://search-your-domain-xyz.region.es.amazonaws.com`\n",
    "   - `OPENSEARCH_USERNAME` / `OPENSEARCH_PASSWORD` – only if your domain uses basic auth.  \n",
    "     If it uses IAM/SigV4, you would instead need `requests-aws4auth` and SigV4 signing.\n",
    "3. Uncomment and run the cell to send the bulk indexing request.\n",
    "\n",
    "If you **cannot** use OpenSearch, it is enough to show the bulk payload you built in the previous cell in your capstone report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd8b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- OPTIONAL: Index into OpenSearch (uncomment and configure if allowed) ----\n",
    "# import requests\n",
    "# from requests.auth import HTTPBasicAuth\n",
    "#\n",
    "# OPENSEARCH_ENDPOINT = \"https://your-domain-endpoint-here\"  # e.g., https://search-capstone-demo-xyz.region.es.amazonaws.com\n",
    "# OPENSEARCH_USERNAME = \"your-username\"\n",
    "# OPENSEARCH_PASSWORD = \"your-password\"\n",
    "#\n",
    "# bulk_url = f\"{OPENSEARCH_ENDPOINT}/{index_name}/_bulk\"\n",
    "# headers = {\"Content-Type\": \"application/x-ndjson\"}\n",
    "#\n",
    "# response = requests.post(\n",
    "#     bulk_url,\n",
    "#     headers=headers,\n",
    "#     data=bulk_payload,\n",
    "#     auth=HTTPBasicAuth(OPENSEARCH_USERNAME, OPENSEARCH_PASSWORD)\n",
    "# )\n",
    "#\n",
    "# print(\"Status code:\", response.status_code)\n",
    "# print(\"Response body:\", response.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec15ea7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you:\n",
    "\n",
    "- **Copied** example MP4 videos from a shared S3 bucket into your own lab bucket.\n",
    "- **Task 1 – Amazon Transcribe**  \n",
    "  - Started one or more Transcribe jobs for the input videos.  \n",
    "  - Retrieved the resulting transcripts from S3, or used a simulated transcript if permissions were blocked.\n",
    "- **Built a transcript DataFrame** with normalized text for further analysis.\n",
    "- **Task 2 – Amazon Comprehend Key Phrases**  \n",
    "  - Uploaded the normalized transcripts as a CSV to S3.  \n",
    "  - Started a Key Phrase Detection job (or simulated it if blocked).  \n",
    "  - Loaded the key phrase results into a DataFrame.\n",
    "- **Task 3 – Amazon OpenSearch (ElasticSearch)**  \n",
    "  - Prepared document structures combining transcripts and key phrases.  \n",
    "  - Built a valid OpenSearch bulk indexing payload (NDJSON format).  \n",
    "  - Optionally, showed how you would send this payload to an OpenSearch domain.\n",
    "\n",
    "This end-to-end workflow is what you can describe and screenshot in your capstone report, noting clearly if any steps used simulated outputs due to AWS Academy IAM restrictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
