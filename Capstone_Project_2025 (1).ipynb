{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 11.08.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Bringing It All Together (2025 Update)\n",
    "\n",
    "This modernized version ensures compatibility with **Python 3.10+**, **boto3 ‚â• 1.34**, and **Amazon OpenSearch 2.x** APIs.\n",
    "\n",
    "You'll transcribe ML-course videos using **Amazon Transcribe**, analyze topics with **Amazon Comprehend**, and visualize them in **OpenSearch Dashboards**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json, uuid, time, os, io, re, requests, tarfile\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import nltk\n",
    "nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\n",
    "\n",
    "bucket = 'c125984a3128017l8216419t1w730335316855-labbucket-jekz3oeugfkv'\n",
    "job_data_access_role = 'arn:aws:iam::730335316855:role/service-role/c125984a3128017l8216419t1w-ComprehendDataAccessRole-DbY1v1fex5lo'\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.client('s3', region_name=region)\n",
    "transcribe = boto3.client('transcribe', region_name=region)\n",
    "comprehend = boto3.client('comprehend', region_name=region)\n",
    "opensearch = boto3.client('opensearch', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ View available videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy them into your bucket for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/ s3://{bucket}/input/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Transcribe the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input files safely\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix='input/')\n",
    "objects = [obj['Key'] for obj in resp.get('Contents', [])]\n",
    "for key in objects:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start transcription for each file\n",
    "output_files = []\n",
    "for obj_key in objects:\n",
    "    if 'temp' in obj_key:\n",
    "        continue\n",
    "\n",
    "    media_uri = f's3://{bucket}/{obj_key}'\n",
    "    job_name = f'transcribe-job-{uuid.uuid4()}'\n",
    "\n",
    "    print(f'üéô Starting transcription for {media_uri}')\n",
    "    transcribe.start_transcription_job(\n",
    "        TranscriptionJobName=job_name,\n",
    "        Media={'MediaFileUri': media_uri},\n",
    "        MediaFormat='mp4',\n",
    "        LanguageCode='en-US',\n",
    "        OutputBucketName=bucket,\n",
    "        Settings={'ShowSpeakerLabels': False, 'ChannelIdentification': False}\n",
    "    )\n",
    "\n",
    "    # Poll until completion\n",
    "    while True:\n",
    "        job = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
    "        status = job['TranscriptionJob']['TranscriptionJobStatus']\n",
    "        if status in ['COMPLETED', 'FAILED']:\n",
    "            print(f'Job {job_name}: {status}')\n",
    "            break\n",
    "        print('.', end='', flush=True)\n",
    "        time.sleep(15)\n",
    "\n",
    "    if status == 'COMPLETED':\n",
    "        uri = job['TranscriptionJob']['Transcript']['TranscriptFileUri']\n",
    "        output_files.append({'Video': obj_key, 'TranscriptUri': uri})\n",
    "\n",
    "print('‚úÖ Transcriptions complete:', len(output_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = []\n",
    "for entry in output_files:\n",
    "    uri = entry['TranscriptUri']\n",
    "    data = requests.get(uri).json()\n",
    "    transcript = data['results']['transcripts'][0]['transcript']\n",
    "    data_rows.append({'Video': entry['Video'], 'Transcription': transcript})\n",
    "\n",
    "df = pd.DataFrame(data_rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(content):\n",
    "    text = re.sub(r'http\\S+', '', content)\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    return text\n",
    "\n",
    "df['Normalized'] = df['Transcription'].apply(normalize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Extract key phrases and entities with Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3', region_name=region)\n",
    "\n",
    "def upload_comprehend_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    key = f\"{folder}/{filename}\"\n",
    "    s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())\n",
    "    print(f'‚úÖ Uploaded to s3://{bucket}/{key}')\n",
    "    return f's3://{bucket}/{key}'\n",
    "\n",
    "input_path = upload_comprehend_s3_csv('comprehend_input.csv', 'capstone/comprehend', df['Normalized'])\n",
    "\n",
    "# Start key phrase detection job\n",
    "kpe_job = comprehend.start_key_phrases_detection_job(\n",
    "    InputDataConfig={'S3Uri': input_path, 'InputFormat': 'ONE_DOC_PER_LINE'},\n",
    "    OutputDataConfig={'S3Uri': f's3://{bucket}/'},\n",
    "    DataAccessRoleArn=job_data_access_role,\n",
    "    JobName=f'kpe-job-{uuid.uuid4()}',\n",
    "    LanguageCode='en'\n",
    ")\n",
    "print('Key phrase job started:', kpe_job['JobId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for job completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = kpe_job['JobId']\n",
    "while True:\n",
    "    status = comprehend.describe_key_phrases_detection_job(JobId=job_id)\n",
    "    state = status['KeyPhrasesDetectionJobProperties']['JobStatus']\n",
    "    if state in ['COMPLETED','FAILED']:\n",
    "        print('Job status:', state)\n",
    "        break\n",
    "    print('.', end='', flush=True)\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create OpenSearch Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ip = 'YOUR.IP.ADDRESS/32'  # e.g., '203.0.113.24/32'\n",
    "access_policy = {\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [{\n",
    "        'Effect': 'Allow',\n",
    "        'Principal': '*',\n",
    "        'Action': 'es:*',\n",
    "        'Resource': '*',\n",
    "        'Condition': {'IpAddress': {'aws:SourceIp': my_ip}}\n",
    "    }]\n",
    "}\n",
    "\n",
    "resp = opensearch.create_domain(\n",
    "    DomainName='nlp-lab',\n",
    "    EngineVersion='OpenSearch_2.11',\n",
    "    ClusterConfig={'InstanceType': 't3.small.search', 'InstanceCount': 1},\n",
    "    AccessPolicies=json.dumps(access_policy)\n",
    ")\n",
    "print('Domain creation started...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    status = opensearch.describe_domain(DomainName='nlp-lab')\n",
    "    if not status['DomainStatus']['Processing']:\n",
    "        break\n",
    "    print('.', end='', flush=True)\n",
    "    time.sleep(30)\n",
    "\n",
    "endpoint = status['DomainStatus']['Endpoint']\n",
    "print(f'‚úÖ OpenSearch ready: https://{endpoint}/_dashboards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Index a few docs into OpenSearch with `opensearch-py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py requests-aws4auth --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, helpers\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "session = boto3.Session()\n",
    "creds = session.get_credentials().get_frozen_credentials()\n",
    "awsauth = AWS4Auth(creds.access_key, creds.secret_key, region, 'es', session_token=creds.token)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': endpoint, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "# Create index if not exists\n",
    "index_name = 'videos'\n",
    "if not client.indices.exists(index=index_name):\n",
    "    client.indices.create(index=index_name)\n",
    "\n",
    "# Bulk load a subset of docs\n",
    "def gendocs(df):\n",
    "    for i, row in df.iterrows():\n",
    "        yield {\n",
    "            '_index': index_name,\n",
    "            '_id': i,\n",
    "            '_source': {\n",
    "                'video': row['Video'],\n",
    "                'transcription': row['Transcription'][:32000],\n",
    "                'normalized': row['Normalized'][:32000]\n",
    "            }\n",
    "        }\n",
    "\n",
    "helpers.bulk(client, gendocs(df))\n",
    "print('‚úÖ Indexed', len(df), 'documents')\n",
    "print('Open the dashboard at: ', f'https://{endpoint}/_dashboards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch.delete_domain(DomainName='nlp-lab')\n",
    "print('Deleting OpenSearch domain...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Congratulations!\n",
    "\n",
    "You have completed the Capstone Project (2025 version). Your notebook now uses all modern AWS SDK methods and APIs safely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}